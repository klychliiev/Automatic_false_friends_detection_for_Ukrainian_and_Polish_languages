{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow\n",
    "1. text preprocessing (tokenization, normalization). test spacy, stanza, pumorphy, llms \n",
    "2. extract potential wordd pairs (orthographic similarity)\n",
    "3. craft the prompt, explain false friends and cognates \n",
    "4. interpret results \n",
    "5. post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_text = 'texts/ukr.txt'\n",
    "pol_text = 'texts/pl.txt'\n",
    "\n",
    "with open(ukr_text, 'r') as f: \n",
    "    ukr = f.read()\n",
    "\n",
    "with open(pol_text, 'r') as f:\n",
    "    pol = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NLP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the Jaro similarity \n",
    "\n",
    "from math import floor \n",
    "\n",
    "def jaro_distance(s1, s2):\n",
    "     \n",
    "    # If the s are equal\n",
    "    if (s1 == s2):\n",
    "        return 1.0\n",
    " \n",
    "    # Length of two s\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    " \n",
    "    # Maximum distance upto which matching\n",
    "    # is allowed\n",
    "    max_dist = floor(max(len1, len2) / 2) - 1\n",
    " \n",
    "    # Count of matches\n",
    "    match = 0\n",
    " \n",
    "    # Hash for matches\n",
    "    hash_s1 = [0] * len(s1)\n",
    "    hash_s2 = [0] * len(s2)\n",
    " \n",
    "    # Traverse through the first\n",
    "    for i in range(len1):\n",
    " \n",
    "        # Check if there is any matches\n",
    "        for j in range(max(0, i - max_dist),\n",
    "                       min(len2, i + max_dist + 1)):\n",
    "             \n",
    "            # If there is a match\n",
    "            if (s1[i] == s2[j] and hash_s2[j] == 0):\n",
    "                hash_s1[i] = 1\n",
    "                hash_s2[j] = 1\n",
    "                match += 1\n",
    "                break\n",
    " \n",
    "    # If there is no match\n",
    "    if (match == 0):\n",
    "        return 0.0\n",
    " \n",
    "    # Number of transpositions\n",
    "    t = 0\n",
    "    point = 0\n",
    " \n",
    "    # Count number of occurrences\n",
    "    # where two characters match but\n",
    "    # there is a third matched character\n",
    "    # in between the indices\n",
    "    for i in range(len1):\n",
    "        if (hash_s1[i]):\n",
    " \n",
    "            # Find the next matched character\n",
    "            # in second\n",
    "            while (hash_s2[point] == 0):\n",
    "                point += 1\n",
    " \n",
    "            if (s1[i] != s2[point]):\n",
    "                t += 1\n",
    "            point += 1\n",
    "    t = t//2\n",
    " \n",
    "    # Return the Jaro Similarity\n",
    "    return (match/ len1 + match / len2 +\n",
    "            (match - t) / match)/ 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download pl_core_news_sm\n",
    "# !python3 -m spacy download uk_core_news_sm\n",
    "# !python3 -m spacy download uk_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "\n",
    "doc = nlp(u\"Ukraina jest najlepszym krajem na świecie, moja Ukraina. Moja matka jest najlepszą kobietą na świecie.\")\n",
    "\n",
    "tokens_dict = {token.text: token.lemma_ for token in doc if token.is_alpha}\n",
    "\n",
    "pl_lemmas = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"uk_core_news_sm\")\n",
    "\n",
    "doc = nlp(u\"Україна - то найкраща країна на всьому світі. Люблю її безмежно. Матка - то важливий жіночий орган.\")\n",
    "\n",
    "tokens_dict = {token.text: token.lemma_ for token in doc if token.is_alpha}\n",
    "\n",
    "uk_lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.is_alpha, token.is_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['україна',\n",
       " 'то',\n",
       " 'найкращий',\n",
       " 'країна',\n",
       " 'на',\n",
       " 'ввесь',\n",
       " 'світ',\n",
       " 'любити',\n",
       " 'вона',\n",
       " 'безмежно',\n",
       " 'матка',\n",
       " 'то',\n",
       " 'важливий',\n",
       " 'жіночий',\n",
       " 'орган']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukraina',\n",
       " 'być',\n",
       " 'dobry',\n",
       " 'kraj',\n",
       " 'na',\n",
       " 'świat',\n",
       " 'mój',\n",
       " 'Ukraina',\n",
       " 'mój',\n",
       " 'matka',\n",
       " 'być',\n",
       " 'najlepszą',\n",
       " 'kobieta',\n",
       " 'na',\n",
       " 'świat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in ./diploma-env/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: polyglot in ./diploma-env/lib/python3.10/site-packages (16.7.4)\n",
      "Requirement already satisfied: icu in ./diploma-env/lib/python3.10/site-packages (0.0.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install Levenshtein polyglot icu tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthographically similar word pairs:\n",
      "('ukraina', 'ukraina')\n",
      "('ukraina', 'ukraina')\n",
      "('kraina', 'ukraina')\n",
      "('kraina', 'ukraina')\n",
      "('na', 'na')\n",
      "('na', 'na')\n",
      "('matka', 'matka')\n",
      "('diskusia', 'dyskusja')\n",
      "('pirogi', 'pierogi')\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from polyglot.downloader import downloader \n",
    "from polyglot.text import Text \n",
    "\n",
    "# Ukrainian and Polish word lists\n",
    "ukrainian_words = ['україна', 'то', 'найкращий', 'країна', 'на', 'ввесь', 'світ', 'любити', 'вона', 'безмежно', 'матка', 'то', 'важливий', 'жіночий', 'орган', \"дискусія\", \"пироги\"]\n",
    "polish_words = ['Ukraina', 'być', 'dobry', 'kraj', 'na', 'świat', 'mój', 'Ukraina', 'mój', 'matka', 'być', 'najlepszą', 'kobieta', 'na', 'świat', \"dyskusja\", \"pierogi\"]\n",
    "\n",
    "uk_trans = [str(Text(word.lower(), hint_language_code='uk').transliterate('en')[0]) for word in ukrainian_words]\n",
    "pl_trans = [str(Text(word.lower(), hint_language_code='pl').transliterate('en')[0]) for word in polish_words]\n",
    "\n",
    "# # Normalize the words to lowercase\n",
    "# ukrainian_words = [word.lower() for word in ukrainian_words]\n",
    "# polish_words = [word.lower() for word in polish_words]\n",
    "\n",
    "# Set a threshold for similarity\n",
    "# Example threshold is set to 3, adjust based on your needs\n",
    "\n",
    "# Find pairs based on Levenshtein distance\n",
    "similar_pairs = []\n",
    "for u_word in uk_trans:\n",
    "    for p_word in pl_trans:\n",
    "        if jaro_distance(u_word, p_word) >= 0.8:\n",
    "            similar_pairs.append((u_word, p_word))\n",
    "\n",
    "# Print the similar pairs\n",
    "print(\"Orthographically similar word pairs:\")\n",
    "for pair in similar_pairs:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthographically similar word pairs:\n",
      "('україна', 'Ukraina')\n",
      "('країна', 'Ukraina')\n",
      "('на', 'na')\n",
      "('матка', 'matka')\n",
      "('дискусія', 'dyskusja')\n",
      "('пироги', 'pierogi')\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from polyglot.downloader import downloader \n",
    "from polyglot.text import Text \n",
    "\n",
    "# Ukrainian and Polish word lists\n",
    "ukrainian_words = ['україна', 'то', 'найкращий', 'країна', 'на', 'ввесь', 'світ', 'любити', 'вона', 'безмежно', 'матка', 'то', 'важливий', 'жіночий', 'орган', \"дискусія\", \"пироги\"]\n",
    "polish_words = ['Ukraina', 'być', 'dobry', 'kraj', 'na', 'świat', 'mój', 'Ukraina', 'mój', 'matka', 'być', 'najlepszą', 'kobieta', 'na', 'świat', \"dyskusja\", \"pierogi\"]\n",
    "\n",
    "uk_trans = {str(Text(word.lower(), hint_language_code='uk').transliterate('en')[0]):word for word in ukrainian_words}\n",
    "pl_trans = {str(Text(word.lower(), hint_language_code='pl').transliterate('en')[0]):word for word in polish_words}\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Find pairs based on Levenshtein distance\n",
    "similar_pairs = []\n",
    "for u_word in uk_trans:\n",
    "    for p_word in pl_trans:\n",
    "        if jaro_distance(u_word, p_word) >= threshold:\n",
    "            similar_pairs.append((uk_trans[u_word], pl_trans[p_word]))\n",
    "\n",
    "# Print the similar pairs\n",
    "print(\"Orthographically similar word pairs:\")\n",
    "for pair in similar_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('україна', 'Ukraina'),\n",
       " ('країна', 'Ukraina'),\n",
       " ('на', 'na'),\n",
       " ('матка', 'matka'),\n",
       " ('дискусія', 'dyskusja'),\n",
       " ('пироги', 'pierogi')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in ./diploma-env/lib/python3.10/site-packages (from openai) (2.7.1)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: tqdm>4 in ./diploma-env/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./diploma-env/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./diploma-env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./diploma-env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in ./diploma-env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./diploma-env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./diploma-env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Installing collected packages: sniffio, h11, distro, httpcore, anyio, httpx, openai\n",
      "Successfully installed anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.25.1 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мир', 'mir'), ('рак', 'rak'), ('магазин', 'magazyn')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pairs = [('мир', 'mir'), ('рак', 'rak'), ('магазин', 'magazyn')]\n",
    "similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='sk-proj-L7S2GeXJ5B1hdpPfT1TVT3BlbkFJF8Iw3SjL6U2IgdvXjnyX'\n",
    ")\n",
    "\n",
    "# Assume you have a function that finds similar words between Ukrainian and Polish texts\n",
    "potential_pairs = similar_pairs\n",
    "\n",
    "def check_false_friends(u_word, p_word):\n",
    "    response = client.chat.completions.create(\n",
    "        messages = [\n",
    "        {\n",
    "        'role':'user',\n",
    "        'content':f\"Are Ukrainian word: '{u_word}' and Polish word: '{p_word}' false friends or cognates? Explain your answer.\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"gpt-4-turbo\",  \n",
    "    )\n",
    "    return response\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('мир', 'mir'): ChatCompletion(id='chatcmpl-9LY8vUmGk6Fm7NHK8LvuUkrizRSux', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \"мир\" and the Polish word \"mir\" can indeed be considered both false friends and cognates, though the specific meanings and usages in each language lead to different interpretations.\\n\\n1. **Cognates**: Both words are derived from the Old Church Slavonic word \"миръ\" which means \"peace\". Old Church Slavonic has influenced many Slavic languages, including Ukrainian and Polish. Therefore, if we look at the origin of the words, they stem from the same root and are thus cognates.\\n\\n2. **False Friends**: Despite having the same origin, the meaning of \"мир\" in contemporary Ukrainian and Polish can differ significantly, leading them to be false friends. In Ukrainian, \"мир\" primarily means \"peace\" (similar to its original Old Church Slavic meaning). However, it can also mean \"world\", in contexts similar to the Russian usage of the same word.\\n\\n   In Polish, however, \"mir\" is an old or literary term that might still be recognized by some speakers but is not in common modern use; its meanings historically included \"peace\" or \"community\". In modern Polish, the common word for \"peace\" is \"pokój\" and for \"world\" is \"świat\".\\n\\nTherefore, while the words are cognates due to their common historical origin, in modern usage, they can act as false friends because \"мир\" in Ukrainian might not be immediately understood as \"mir\" in Polish, where the latter isn’t regularly used in contemporary language. Additionally, the contexts and frequencies of their uses differ, adding to the confusion for speakers of both languages.', role='assistant', function_call=None, tool_calls=None))], created=1714923161, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_3450ce39d5', usage=CompletionUsage(completion_tokens=335, prompt_tokens=32, total_tokens=367)), ('рак', 'rak'): ChatCompletion(id='chatcmpl-9LY9AvrfqUS05avDtDexI3iRmJnLn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \\'рак\\' and the Polish word \\'rak\\' are not false friends; they are cognates. This means they both originate from a common ancestral language and share a similar meaning, as well as a similar form.\\n\\nIn Ukrainian, \\'рак\\' primarily means \"crayfish\" and also \"cancer\" (the disease) as a secondary meaning. In Polish, \\'rak\\' has the same dual meanings: “crayfish” and \"cancer\" (disease). This dual meaning is not coincidental but is reflective of the historical use of the term in Proto-Slavic (the common ancestor of both Ukrainian and Polish), from which the words in both languages are derived.\\n\\nProto-Slavic *rakъ originally meant \"crayfish,\" and the metaphorical use to describe the disease cancer likely comes from the crab-like shape and spreading nature of malignant tumors, akin to the imagery used in English and other languages (derived from Latin \\'cancer,\\' meaning \"crab\").\\n\\nThus, being derived from the same Proto-Slavic root and retaining the same meanings in both languages, Ukrainian \\'рак\\' and Polish \\'rak\\' are cognates.', role='assistant', function_call=None, tool_calls=None))], created=1714923176, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_ea6eb70039', usage=CompletionUsage(completion_tokens=244, prompt_tokens=33, total_tokens=277)), ('магазин', 'magazyn'): ChatCompletion(id='chatcmpl-9LY9JIFUpMiwFmiFYLrh63rG5agqg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \"магазин\" (magazyn) and the Polish word \"magazyn\" are indeed cognates, not false friends. Both words share a similar meaning as well as a common etymological origin, which means that they derive from the same linguistic source.\\n\\n1. **Meaning**: In both Ukrainian and Polish, these words mainly denote a place where goods are stored or sold - a store or a warehouse. This common meaning shows that there isn\\'t a misleading difference in the conceptualization that would categorize them as false friends. False friends are words that sound or look similar in two languages but differ significantly in meaning.\\n\\n2. **Etymology**: Both words trace back to the French word \"magasin,\" which in turn comes from the Italian \"magazzino,\" meaning \"storehouse.\" This word derives from the Arabic \"makhāzin\" (plural of \"makhzan,\" meaning \"storehouse\"), which is ultimately derived from the verb \"khazana,\" meaning \"to store.\" The pathway of etymological transmission likely involved the borrowing of the word from French into both Polish and Ukrainian, or through another intermediate language, under the influence of trade and historical linkages in Europe.\\n\\nGiven this shared etymological background and similar meanings in both languages, \"магазин\" in Ukrainian and \"magazyn\" in Polish are very clearly cognates.', role='assistant', function_call=None, tool_calls=None))], created=1714923185, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_3450ce39d5', usage=CompletionUsage(completion_tokens=285, prompt_tokens=36, total_tokens=321))}\n"
     ]
    }
   ],
   "source": [
    "results = {pair: check_false_friends(*pair) for pair in potential_pairs}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('мир',\n",
       "  'mir'): ChatCompletion(id='chatcmpl-9LY8vUmGk6Fm7NHK8LvuUkrizRSux', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \"мир\" and the Polish word \"mir\" can indeed be considered both false friends and cognates, though the specific meanings and usages in each language lead to different interpretations.\\n\\n1. **Cognates**: Both words are derived from the Old Church Slavonic word \"миръ\" which means \"peace\". Old Church Slavonic has influenced many Slavic languages, including Ukrainian and Polish. Therefore, if we look at the origin of the words, they stem from the same root and are thus cognates.\\n\\n2. **False Friends**: Despite having the same origin, the meaning of \"мир\" in contemporary Ukrainian and Polish can differ significantly, leading them to be false friends. In Ukrainian, \"мир\" primarily means \"peace\" (similar to its original Old Church Slavic meaning). However, it can also mean \"world\", in contexts similar to the Russian usage of the same word.\\n\\n   In Polish, however, \"mir\" is an old or literary term that might still be recognized by some speakers but is not in common modern use; its meanings historically included \"peace\" or \"community\". In modern Polish, the common word for \"peace\" is \"pokój\" and for \"world\" is \"świat\".\\n\\nTherefore, while the words are cognates due to their common historical origin, in modern usage, they can act as false friends because \"мир\" in Ukrainian might not be immediately understood as \"mir\" in Polish, where the latter isn’t regularly used in contemporary language. Additionally, the contexts and frequencies of their uses differ, adding to the confusion for speakers of both languages.', role='assistant', function_call=None, tool_calls=None))], created=1714923161, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_3450ce39d5', usage=CompletionUsage(completion_tokens=335, prompt_tokens=32, total_tokens=367)),\n",
       " ('рак',\n",
       "  'rak'): ChatCompletion(id='chatcmpl-9LY9AvrfqUS05avDtDexI3iRmJnLn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \\'рак\\' and the Polish word \\'rak\\' are not false friends; they are cognates. This means they both originate from a common ancestral language and share a similar meaning, as well as a similar form.\\n\\nIn Ukrainian, \\'рак\\' primarily means \"crayfish\" and also \"cancer\" (the disease) as a secondary meaning. In Polish, \\'rak\\' has the same dual meanings: “crayfish” and \"cancer\" (disease). This dual meaning is not coincidental but is reflective of the historical use of the term in Proto-Slavic (the common ancestor of both Ukrainian and Polish), from which the words in both languages are derived.\\n\\nProto-Slavic *rakъ originally meant \"crayfish,\" and the metaphorical use to describe the disease cancer likely comes from the crab-like shape and spreading nature of malignant tumors, akin to the imagery used in English and other languages (derived from Latin \\'cancer,\\' meaning \"crab\").\\n\\nThus, being derived from the same Proto-Slavic root and retaining the same meanings in both languages, Ukrainian \\'рак\\' and Polish \\'rak\\' are cognates.', role='assistant', function_call=None, tool_calls=None))], created=1714923176, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_ea6eb70039', usage=CompletionUsage(completion_tokens=244, prompt_tokens=33, total_tokens=277)),\n",
       " ('магазин',\n",
       "  'magazyn'): ChatCompletion(id='chatcmpl-9LY9JIFUpMiwFmiFYLrh63rG5agqg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ukrainian word \"магазин\" (magazyn) and the Polish word \"magazyn\" are indeed cognates, not false friends. Both words share a similar meaning as well as a common etymological origin, which means that they derive from the same linguistic source.\\n\\n1. **Meaning**: In both Ukrainian and Polish, these words mainly denote a place where goods are stored or sold - a store or a warehouse. This common meaning shows that there isn\\'t a misleading difference in the conceptualization that would categorize them as false friends. False friends are words that sound or look similar in two languages but differ significantly in meaning.\\n\\n2. **Etymology**: Both words trace back to the French word \"magasin,\" which in turn comes from the Italian \"magazzino,\" meaning \"storehouse.\" This word derives from the Arabic \"makhāzin\" (plural of \"makhzan,\" meaning \"storehouse\"), which is ultimately derived from the verb \"khazana,\" meaning \"to store.\" The pathway of etymological transmission likely involved the borrowing of the word from French into both Polish and Ukrainian, or through another intermediate language, under the influence of trade and historical linkages in Europe.\\n\\nGiven this shared etymological background and similar meanings in both languages, \"магазин\" in Ukrainian and \"magazyn\" in Polish are very clearly cognates.', role='assistant', function_call=None, tool_calls=None))], created=1714923185, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_3450ce39d5', usage=CompletionUsage(completion_tokens=285, prompt_tokens=36, total_tokens=321))}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = [('Україна', 'Ukraina'), ('на', 'na'), ('матка', 'matka'),('пироги','pierogi'),('склеп','sklep')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Україна', 'Ukraina'),\n",
       " ('на', 'na'),\n",
       " ('матка', 'matka'),\n",
       " ('пироги', 'pierogi'),\n",
       " ('склеп', 'sklep')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='sk-proj-L7S2GeXJ5B1hdpPfT1TVT3BlbkFJF8Iw3SjL6U2IgdvXjnyX'\n",
    ")\n",
    "\n",
    "# Assume you have a function that finds similar words between Ukrainian and Polish texts\n",
    "potential_pairs = similar_pairs\n",
    "\n",
    "def check_false_friends(word_pairs):\n",
    "\n",
    "    template = \"\"\"Task: Determine if the following pairs of Ukrainian and Polish words are false friends (different meanings) or cognates (similar meanings).\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "    \"word_pairs\": [\n",
    "        {\n",
    "        \"words\": [\"рак\", \"rak\"],\n",
    "        \"status\": \"Cognates\"\n",
    "        },\n",
    "        {\n",
    "        \"words\": [\"магазин\", \"magazyn\"],\n",
    "        \"status\": \"False friends\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "\n",
    "    Return the valid JSON as above for the following Ukrainian-Polish word pairs: \"\"\" \n",
    "\n",
    "    template += str(word_pairs)\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "        messages = [\n",
    "            {\n",
    "            'role':'user',\n",
    "            'content':template\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = check_false_friends(similar_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mres\u001b[49m\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_pairs': [{'words': ['Україна', 'Ukraina'], 'status': 'Cognates'}, {'words': ['на', 'na'], 'status': 'Cognates'}, {'words': ['матка', 'matka'], 'status': 'False friends'}, {'words': ['пироги', 'pierogi'], 'status': 'Cognates'}, {'words': ['склеп', 'sklep'], 'status': 'False friends'}]}\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "json_str = res.choices[0].message.content \n",
    "\n",
    "data = json.loads(json_str)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Task: Determine if the following pairs of Ukrainian and Polish words are false friends (different meanings) or cognates (similar meanings).\n",
    "\n",
    "Examples:\n",
    "1. Ukrainian: 'рак', Polish: 'rak' -> Cognates\n",
    "2. Ukrainian: 'магазин', Polish: 'magazyn' -> False Friends\n",
    "\n",
    "Classify the following pairs:\"\"\" \n",
    "\n",
    "for i, (ukr_word, pl_word) in enumerate(similar_pairs):\n",
    "    template += f\"{i}. Ukrainian: '{ukr_word}', Polish: '{pl_word}' -> \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the following pairs of Ukrainian and Polish words are false friends (different meanings) or cognates (similar meanings).\n",
      "\n",
      "Examples:\n",
      "1. Ukrainian: 'рак', Polish: 'rak' -> Cognates\n",
      "2. Ukrainian: 'магазин', Polish: 'magazyn' -> False Friends\n",
      "\n",
      "Classify the following pairs:0. Ukrainian: 'мир', Polish: 'mir' -> \n",
      "1. Ukrainian: 'рак', Polish: 'rak' -> \n",
      "2. Ukrainian: 'магазин', Polish: 'magazyn' -> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "print(jaro_distance('ukraina', 'kobieta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package transliteration2.uk to\n",
      "[polyglot_data]     /home/klychliiev/polyglot_data...\n",
      "[polyglot_data]   Package transliteration2.uk is already up-to-date!\n",
      "[polyglot_data] Downloading package transliteration2.pl to\n",
      "[polyglot_data]     /home/klychliiev/polyglot_data...\n"
     ]
    }
   ],
   "source": [
    "!polyglot download transliteration2.uk\n",
    "!polyglot download transliteration2.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.downloader import downloader \n",
    "from polyglot.text import Text \n",
    "\n",
    "uk_blob = ukrainian_words[1]\n",
    "pl_blob = polish_words[1]\n",
    "\n",
    "uk_text = Text(uk_blob, hint_language_code='uk')\n",
    "uk_translitareted = str(uk_text.transliterate('en')[0])\n",
    "\n",
    "\n",
    "pl_text = Text(pl_blob, hint_language_code='pl')\n",
    "pl_translitareted = str(pl_text.transliterate('en')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'być'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_translitareted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 14:04:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 5.36MB/s]                    \n",
      "2024-05-05 14:04:25 INFO: Downloaded file to /home/klychliiev/stanza_resources/resources.json\n",
      "2024-05-05 14:04:25 INFO: Loading these models for language: uk (Ukrainian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | iu          |\n",
      "| mwt       | iu          |\n",
      "| lemma     | iu_nocharlm |\n",
      "===========================\n",
      "\n",
      "2024-05-05 14:04:25 INFO: Using device: cpu\n",
      "2024-05-05 14:04:25 INFO: Loading: tokenize\n",
      "2024-05-05 14:04:25 INFO: Loading: mwt\n",
      "2024-05-05 14:04:25 INFO: Loading: lemma\n",
      "2024-05-05 14:04:25 INFO: Done loading processors!\n",
      "2024-05-05 14:04:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 7.20MB/s]                    \n",
      "2024-05-05 14:04:26 INFO: Downloaded file to /home/klychliiev/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-pl/resolve/v1.8.0/models/tokenize/pdb.pt: 100%|██████████| 638k/638k [00:00<00:00, 2.36MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-pl/resolve/v1.8.0/models/mwt/pdb.pt: 100%|██████████| 622k/622k [00:00<00:00, 2.05MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-pl/resolve/v1.8.0/models/lemma/pdb_nocharlm.pt: 100%|██████████| 5.91M/5.91M [00:02<00:00, 2.60MB/s]\n",
      "2024-05-05 14:04:32 INFO: Loading these models for language: pl (Polish):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdb          |\n",
      "| mwt       | pdb          |\n",
      "| lemma     | pdb_nocharlm |\n",
      "============================\n",
      "\n",
      "2024-05-05 14:04:32 INFO: Using device: cpu\n",
      "2024-05-05 14:04:32 INFO: Loading: tokenize\n",
      "2024-05-05 14:04:32 INFO: Loading: mwt\n",
      "2024-05-05 14:04:32 INFO: Loading: lemma\n",
      "2024-05-05 14:04:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza \n",
    "\n",
    "uk_nlp = stanza.Pipeline(lang=\"uk\", processors='tokenize,mwt,lemma')\n",
    "pl_nlp = stanza.Pipeline(lang=\"pl\", processors='tokenize,mwt,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc = uk_nlp(u\"Україна - то найкраща країна на всьому світі. Люблю її безмежно.\")\n",
    "print(*[f'{word.text}\\t{word.lemma}\\n' for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukraina\tukraina\n",
      " jest\tbyć\n",
      " najlepszym\tdobry\n",
      " krajem\tkraj\n",
      " na\tna\n",
      " świecie\tświat\n",
      " .\t.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = pl_nlp(u\"Ukraina jest najlepszym krajem na świecie.\")\n",
    "print(*[f'{word.text}\\t{word.lemma}\\n' for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uk-core-news-trf==3.7.2\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/uk_core_news_trf-3.7.2/uk_core_news_trf-3.7.2-py3-none-any.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hCollecting spacy-curated-transformers<0.3.0,>=0.2.0\n",
      "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in ./diploma-env/lib/python3.10/site-packages (from uk-core-news-trf==3.7.2) (2.0.1)\n",
      "Requirement already satisfied: pymorphy3-dicts-uk in ./diploma-env/lib/python3.10/site-packages (from uk-core-news-trf==3.7.2) (2.4.1.1.1663094765)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in ./diploma-env/lib/python3.10/site-packages (from uk-core-news-trf==3.7.2) (3.7.4)\n",
      "Collecting protobuf<3.21.0\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3-dicts-ru in ./diploma-env/lib/python3.10/site-packages (from pymorphy3>=1.0.0->uk-core-news-trf==3.7.2) (2.4.417150.4580142)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in ./diploma-env/lib/python3.10/site-packages (from pymorphy3>=1.0.0->uk-core-news-trf==3.7.2) (0.7.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (4.66.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (24.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.7.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.31.0)\n",
      "Requirement already satisfied: setuptools in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (59.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.0.10)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.22.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.4.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (8.2.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.1.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./diploma-env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.0.8)\n",
      "Collecting curated-transformers<0.2.0,>=0.1.0\n",
      "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in ./diploma-env/lib/python3.10/site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (2.3.0)\n",
      "Collecting curated-tokenizers<0.1.0,>=0.0.9\n",
      "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022\n",
      "  Using cached regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "Requirement already satisfied: language-data>=1.2 in ./diploma-env/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./diploma-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./diploma-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./diploma-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./diploma-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./diploma-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./diploma-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./diploma-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./diploma-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./diploma-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (3.14.0)\n",
      "Requirement already satisfied: fsspec in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.3.0 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (2.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.105)\n",
      "Requirement already satisfied: networkx in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./diploma-env/lib/python3.10/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (1.12)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./diploma-env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (12.4.127)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./diploma-env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./diploma-env/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./diploma-env/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./diploma-env/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->uk-core-news-trf==3.7.2) (1.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./diploma-env/lib/python3.10/site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->uk-core-news-trf==3.7.2) (1.3.0)\n",
      "Installing collected packages: regex, protobuf, curated-tokenizers, curated-transformers, spacy-curated-transformers, uk-core-news-trf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.26.1\n",
      "    Uninstalling protobuf-5.26.1:\n",
      "      Successfully uninstalled protobuf-5.26.1\n",
      "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 protobuf-3.20.3 regex-2024.4.28 spacy-curated-transformers-0.2.2 uk-core-news-trf-3.7.2\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('uk_core_news_trf')\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m spacy download pl_core_news_sm\n",
    "!python3 -m spacy download uk_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Using cached stanza-1.8.2-py3-none-any.whl (990 kB)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m189.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:41\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting protobuf>=3.15.0\n",
      "  Using cached protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "Requirement already satisfied: numpy in ./diploma-env/lib/python3.10/site-packages (from stanza) (1.22.4)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm in ./diploma-env/lib/python3.10/site-packages (from stanza) (4.66.4)\n",
      "Requirement already satisfied: requests in ./diploma-env/lib/python3.10/site-packages (from stanza) (2.31.0)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m267.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Collecting triton==2.3.0\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m319.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:12\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m212.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:28\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m243.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m807.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:01:36\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m302.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m141.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:12\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m774.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m261.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m286.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:14\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m283.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:23\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m392.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m388.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./diploma-env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m243.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m299.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in ./diploma-env/lib/python3.10/site-packages (from requests->stanza) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./diploma-env/lib/python3.10/site-packages (from requests->stanza) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./diploma-env/lib/python3.10/site-packages (from requests->stanza) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./diploma-env/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./diploma-env/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m146.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, toml, sympy, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, emoji, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, stanza\n",
      "Successfully installed emoji-2.11.1 filelock-3.14.0 fsspec-2024.3.1 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 protobuf-5.26.1 stanza-1.8.2 sympy-1.12 toml-0.10.2 torch-2.3.0 triton-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
